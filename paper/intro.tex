\section{Introduction}


Logical rules are a popular knowledge representation language in many domains. They represent %background 
domain knowledge, encode information that can be derived from given facts in a compact form, and allow for logical reasoning.
For example, given facts $\expred{parent}(\exconst{ann},\exconst{bob})$ and $\expred{parent}(\exconst{bob},\exconst{dan})$, the \emph{Datalog rule} 
$\expred{grandparent}(X,Z)\ass\expred{parent}(X,Y),\expred{parent}(Y,Z)$, encodes the fact $\expred{grandparent}(\exconst{ann},\exconst{dan})$ and describes its dependency on the other facts. Moreover, if the data grows and new facts are added, we can automatically derive new knowledge.
% 
Since rule formulation is complex and requires domain expertise,
\emph{rule learning} \cite{Raedt-08-Logical-and-relational-learning,Fuernkranz+-12-Foundations-of-Rule-Learning} %,SteGaHo-RR18:overview} 
has been an active research in AI for a long time %However, the classical approaches do not scale struggled with the  and has r
and recently revived with the application of deep learning. % approaches.

In the area of %\emph
{inductive logic programming}~(ILP) \cite{Muggleton-NGC91:ilp}, %this tedious task is solved by inducing 
the goal is to induce logical rules (also, logic programs) based on example facts. 
Classical ILP systems such as FOIL \cite{Quinlan-ML90:foil} and Progol \cite{Muggleton-NGC95:progol} usually apply exhaustive algorithms to mine rules for the given data and either require negative facts as
counter-examples or assume a closed world (for an overview of classical ILP systems see Table~2 in \cite{SteGaHo-RR18:overview}).
The \emph{closed-world assumption} (CWA) basically states that all facts % given are considered as true and 
that are not explicitly given as true are assumed to be false.
%, if given facts match a rule's conditions, then the consequences of the rule have to be also true in the real world.

Today, however, knowledge graphs (KGs) with their often incomplete, noisy, heterogeneous, and, especially, large amounts of data raise %completely 
new problems and require new solutions.
For instance, real data most often only partially satisfies the CWA %this assumption 
and does not contain counter-examples. Moreover, in an open world, absent facts cannot be considered as counter-examples either since they are not considered as false.
Therefore, the successor systems, with AMIE+ \cite{Galarraga+-VLDBJ15:amiep} and RDF2Rules \cite{WangLi-CoRR15:rdf2rules} as the most prominent representatives, 
assume the data to be only partially complete and
focus on rule learning in the sense of mining patterns that occur frequently in the data.
% This is the central challenge of our
% setting: to provide counterexamples for the rule mining.
% Since they lack scalability,
Furthermore, they implement advanced optimization approaches that make them applicable in wider scenarios. %eg use pruning, in-memory indexing
% 
In this way, they address already many of the issues that arise with today's knowledge graphs. Yet, their processing is still exhaustive.

% Today, however, knowledge graphs with their often incomplete, noisy, heterogeneous, and, especially, large amounts of data raise %completely 
% new problems and require new solutions.

%Ho+-ISWC18:guided-by-embedding,
%\emph{rule learning} \cite{Raedt-08-Logical-and-relational-learning,Fuernkranz+-12-Foundations-of-Rule-Learning},
Recently, {neural rule learning} approaches have been proposed  \cite{YaYaCo-NIPS17:neurallp,RoR-NIPS17,EGre-jair18:learning-explanatory-rules,Minervini+-NAMPI18:ntp-at-scale,OWaWa-IJCAI18:scalable-rule-learning,Campero+-corr18,Dong+-ICLR19:nlms} and they might be a good alternative %for learning rules % representations} 
since deep learning has coped with vast amounts of noisy and heterogeneous data already.
% have shown to provide solutions since 
The proposed solutions consider vector or matrix embeddings of symbols, facts and/or rules, and model inference using differentiable operations such as vector addition and matrix composition. However, the proposed methods are still premature: they only learn certain kinds of rules or lack scalability (e.g., because they search the entire rule space) and hence cannot compete with established rule mining systems such as AMIE+ yet, as shown in \cite{OWaWa-IJCAI18:scalable-rule-learning}, for example. 
%, or simply produce unsatisfactory results.
Further, the reported results are questionable in terms of generalization:
% , especially w.r.t.\ practice: %transferability to practice: 
the test datasets (i.e., facts and the rules to be learned) either resemble toy examples that neither cover the various kinds of possible dependencies in rule sets nor allow for testing scalability, or they do not contain any rules at all because there simply are no rules yet for the popular, publicly available KGs (see, e.g., the evaluations in \cite{EGre-jair18:learning-explanatory-rules} and \cite{OWaWa-IJCAI18:scalable-rule-learning}).
%daria: To the best of our knowledge, existing methods are purely statistics-based, i.e., they reduce the rule learning problem to algebraic operations on neural-embedding-based representations of a given KG
\veronika{not 1000\% sure (doesn't seem so), can Dong et al %\cite{Dong+-ICLR19:nlms} 
output the rules? if not, delete reference above}

We are hence in a chicken-and-egg situation missing rules for %today's 
KGs in practice in order to learn new rules over them. Observe that the general non-availability of rules is also the reason why there are no neural rule learning approaches as we would expect: systems trained on both a number of KGs and relevant rules over them, which are able to suggest rules over new KGs, containing facts that were not seen during training; %(i.e., (vs.\ 
the existing systems are trained on a single KG alone and suggest rules for exactly that KG.
%learning also from rules instead of from just facts.

In this paper,
% we adopt the terminology of \cite{Raedt-08-Logical-and-relational-learning,Fuernkranz+-12-Foundations-of-Rule-Learning,SteGaHo-RR18:overview} and summarize the above approaches under the term \emph{rule learning}.
%we propose new, synthetic datasets for rule learning which address the above mentioned shortcomings. Further, 
% 
present the tool \tool (\textbf{Ru}le Learning \textbf{Da}tasets) for generating datasets containing both facts and rules, and for evaluating rule learning systems. % to solve this problem.
%These datasets contain both facts and rules. % modeling different % real-world 
% scenarios.
\tool is highly parameterizable; for instance, the number of constants, predicates, facts, consequences of rules (i.e., determining to which extend a dataset satisfies the closed-world assumption), and noise (e.g., wrong consequences) in the data and the kinds of dependencies between rules can be selected freely.
Moreover, \tool allows for assessing the performance of rule learning systems given their results by computing classical metrics, such as Herbrand distance, and measures that are popular with today's neural approaches, such as Hits@n.

In summary, our contributions are as follows:
\begin{itemize}
\item We propose new, synthetic datasets for rule learning which have been generated by our tool \tool and overcome the above mentioned shortcomings of existing datasets (Section~\ref{sec:description}).
\item We describe \tool in detail (Sections~\ref{sec:generation} and \ref{sec:evaluation}) and make it available for the public: $<$link-in-final-version$>$.
\item We evaluate representatives of the different types of rule learning systems on our datasets (Section~\ref{sec:experiments}).
\end{itemize}
% The paper is structured as follows. In Section~\ref{sec:motivation}, we start with an ILP example introducing the problem and motivate our work by pointing out the problems with existing evaluations %related work 
%  In Section~\ref{sec:description}, we then propose new datasets generated by our tool, which is described in Sections~\ref{sec:generation} and \ref{sec:evaluation} subsequently. Section~\ref{sec:experiments} describes our experiments. %evaluate different systems on our data and show ???.

%describe the difficulties with


%TODO mention learning from NL, probabilistic and other others from darias






% \veronika{related work: the other datasets, related tools for dataset generation. maybe also other evaluation measures.}%also sth like daria's evaluation approach rules learned over complete vs over partial KG
%ijcai18: To estimate the predictive power of the corpus of mined rules, we eliminated from each benchmark 30% of its facts (up to 5K facts) involving the target predicates and checked how many facts (including the eliminated ones) can be predi- cated by applying mined rules on the remaining facts.
%many use only chain like rules


%rules are of course useful, applied in...
%manual formulation bottleneck
%indeed, in a world full of data, eg kgs of google, wikidata?, yago
%task cannot be completed 
%neural ilp approaches to date work using embeddings for ..., differentiable versions of unification etc
%evalk usually not based onrules

%we target systems that lear explicit rule representations using neural approaches
%observe that ilp setting artificial does not directly help in practice
%real more ambitious goal
%make truly neural
%to the ebst of ur knowledge such an amount of datasets


%goal learning rules
%in any incomplete large-scale KG setting ie independentlyof dom
%dev of trad systems (were applied in specia use cases!) stopped: approach not scalable - exhaustive search in space of ...
%in fact, new proposed systs/approaches in the literature use DL for learning/produce rule represents 
% new: darias related (work on kb embdeeings is only coarsely related? - 1. try to find embedding completing missing 2 then get rules)
%evans
%sebast and link
%however to advance neural ILP and to develop systems that are applicable in arbitrary incomplete large-scale KG setting, independentlyof dom,

%we can vary ie our appraoch is flexible and we can adapt and
%combine/adhere to systems restrictions or go more into real world setting eg for dev of new approaches
%our eval reflects this where we eval various approaches with variations of same data

