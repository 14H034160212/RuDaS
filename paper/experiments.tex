\section{Experiments}\label{sec:experiments}


\begin{table*}[h!]
    \centering
    \begin{tabular}{|l|l|l|c|c|c|c|c|}
    \hline
     Name & Size & Category   & \#Facts&\#Rules & \#Predicates & \#Constants \\%& Depth of  \\
          % &  & Category &&&&\\%& Rule Graph \\
     \hline
     CHAIN-XS & tiny & Chain &76&2&9&51\\%train: size, 76 , predicates, 9 , constants, 51
     RDG-XS & tiny & R-DG &63&3&9&35  \\%train: size, 63 , predicates, 9 , constants, 35
     DRDG-XS & tiny & DR-DG & 97&3&11 &67 \\%size, 97 , predicates, 11 , constants, 67
     CHAIN-S & small & Chain & 677&2& 11 & 287\\%size, 677 , predicates, 11 , constants, 287
     RDG-S & small & R-DG &634&3&11& 379 \\%size, 634 , predicates, 11 , constants, 379
     DRDG-S & small & DR-DG & 790&3&11 &  301\\\hline%ize, 790 , predicates, 11 , constants, 301
    \end{tabular}
    \caption{Overview of generated datasets with rule graphs of depth two.}
    \label{tab:datasets2}
\end{table*}


\begin{table*}[t!]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|c|}
    \hline
    %  Name & Size & Rules   & \#Facts&\#Rules & \#Predicates & \#Constants & Depth of  \\
    %       &  & Category &&&&& Rule Graph \\
    %  \hline
     &  EVEN & S-2-1 N &  S-2-2 N &  XS-1 N &  S-2-1 E &  S-2-2 E &  XS-1 E \\\hline
     FOIL & \bf{1.0} & 0.0052 [\bf{1.0}] & 0.0 & 0.0 & 0.575 [\bf{0.982}] &0.0333 [\bf{1.0}] & \bf{0.816} [\bf{1.0}] \\\hline
     Amie  & - & \bf{0.955} & \bf{1.0} & 0.7029& \bf{0.955} & \bf{0.714} & 0.7029 \\\hline
     Neural LP  & - & 0.0 & 0.0 & 0.234  & 0.0 & 0.0& 0.7029 \\\hline
     NTP  & \bf{1.0} & 0.382 &0.03 & \bf{1.0} & 0.382 &0.031 & 0.0 \\\hline
    \end{tabular}
    \caption{\centering Herbrand accuracy for simple datasets (CHAIN). \\ Standard confidence is reported in parenthesis when different from Herbrand accuracy.}
    \label{tab:results}
\end{table*}

%\begin{table*}[t!]
%    \centering
%    \begin{tabular}{|l|c|c|c|c|c|c|c|}
%    \hline
%     &   EVEN & S-2-1 N &  S-2-2 N &  XS-1 N &  S-2-1 E &  S-2-2 E &  XS-1 E \\\hline
%     FOIL  & \bf{1.0} & \bf{1.0} & 0.0 & 0.0&  \bf{0.982} & \bf{1.0} & \bf{1.0}  \\\hline
%     Amie & - & 0.955 & \bf{1.0} & 0.7029 & 0.955 &0.714 & 0.7029 \\\hline
%     Neural LP & - & 0.0 & 0.0 &0.234 &0.0 & 0.0 & 0.7029 \\\hline
%     NTP & \bf{1.0} &0.382 & 0.03& \bf{1.0} & 0.382& 0.031& 0.0  \\\hline
%    \end{tabular}
%    \caption{Standard confidence for simple and existing datasets (CHAIN)}
%    \label{tab:results}
%\end{table*}

\begin{table*}[t!]
    \centering
    \begin{tabular}{|l|c|c|c|c|}
    \hline
     &  CHAIN-S-2 & CHAIN-S-2-0 &  CHAIN-S-3 &  CHAIN-XS-2 \\\hline
     FOIL & 0.608 [\bf{0.954}] & 0.354 [0.784] & 0.654 [\bf{0.958}] & 0.329 [0.350] \\\hline
     Amie & \bf{0.6262} & \bf{0.6981} & \bf{0.8860} & \bf{0.8667} \\\hline
     Neural LP & 0.0021 [0.0022] & 0.0 & 0.0312 [0.0318]& 0.0533\\\hline
     NTP  &0.0 & 0.0064 & 0.0225 [0.0256] & 0.1610 [0.1755] \\\hline
    \end{tabular}
    \caption{\centering Herbrand accuracy for binary datasets (CHAIN)\\ Standard confidence is reported in parentesis when different from Herbrand accuracy.}
    \label{tab:results}
\end{table*}


\begin{table*}[t!]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|}
    \hline
     &  DRDG-S-2 &  DRDG-S-3 &  DRDG-XS-2 & RDG-S-2 & RDG-S-3 & RDG-XS-2\\\hline
     FOIL &0.193 [0.783] & 0.0 & \bf{0.703} [\bf{0.734}] & 0.0 & 0.515 [\bf{0.911}]& \bf{0.496} [\bf{0.624}] \\\hline
     Amie  &  & \bf{0.4898} & 0.0064 & \bf{0.7137} & & 0.2208 \\\hline
     Neural LP & 0.0080 [0.0083]& 0.0816 [0.0857] &0.0649 [0.0671] & 0.0741 [0.0831] & 0.0482 [0.07958] &0.0029 [0.0031] \\\hline
     NTP & & 0.1801 [0.2068]& 0.1236 & 0.1429 & 0.0 &0.0073\\\hline
    \end{tabular}
    \caption{\centering Herbrand accuracy for binary datasets (DRDG and RDG)\\ Standard confidence is reported in parentesis when different from Herbrand accuracy.}
    \label{tab:results}
\end{table*}

We ran experiments with representatives of the different types of rule learning systems using the datasets described in Section~\ref{sec:description}. In this section, we report and discuss the results.

\subsection{Systems}
We evaluated the following systems.

\textbf{FOIL.} \cite{Quinlan-ML90:foil}
A traditional ILP system whose rule construction is completely driven by positive and negative examples in that the rule bodies are iteratively refined (i.e., atoms are added) until none of the negative examples can be derived by the rule anymore -- though exceptions are possible. The process is guided by custom heuristics based on the examples.
% : after the selection of the head atom, body atoms are iteratively added until none of the negative examples
%progol:
% It combines \emph{inverse entailment} 
% with general-to-specific search through a refinement graph. 
% % Inverse Entailment is used with mode declarations to derive the most-specific clause within the mode language which entails a given example. This clause is used to guide a refinement-graph search. Unlike the searches of Shapiro's MIS and Quinlan's FOIL, Progol's search is efficient and has a 
% The search has been proven to return a solution having the maximum "compression" in the search-space. 
% \veronika{explain Inverse Entailment and compression}
% \veronika{alternatively use FOIL. I think one of the old approaches where we probably get not too strong results is enough because there is not really a point to running `bad' systems, no?}

\textbf{AMIE+.} \cite{Galarraga+-VLDBJ15:amiep} A system mining rules systematically by considering rules with a single body atom and then iteratively refining them %by adding body atoms 
based on custom confidence and coverage measures.
For medium-sized KBs, it has been shown to be several orders of magnitude faster than other state-of-the-art systems. 
%We use the preconfigured parameters but set .. in order
% \veronika{sketch approach} mention converage confidence refinement, pruning

% AMIE also requires the rules to be closed. A variable in a rule is closed if it appears at least twice
% in the rule. A rule is closed if all its variables are
% closed. The restriction to closed rules avoids mining
% rules that predict merely the existence of a fact, as in
% diedIn(x, y) ⇒ ∃z : wasBornIn(x, z).
% AMIE omits also reflexive rules, i.e., rules with

\textbf{Neural LP.} \cite{YaYaCo-NIPS17:neurallp} One of the first end-to-end differentiable approaches. 
\veronika{one sentence about processing (matrix multiplication?). I do not fully understand it, find the description somehow vague "based on tensorlog"..}
Neural LP is special in that it learns
a distribution over logical rules, instead of
an approximation to a particular rule, such that its inference is based on rules that have these confidences attached as weights.


% , and this distributiom During
% inference time.

\textbf{NTP.} \cite{RoR-NIPS17} Similarly, a recent neural approach that recursively constructs a network which derives the training examples through backward chaining.
It is based on learning vector embeddings of symbols and represents symbolic unification by computationally combining them.
%based on using embeddings for ... 
%\veronika{evaluate one traditional ILP system and the neural approaches we get hold of}
%riedel
%mention that we did not get hold of the other systems
%Complex[7] as link prediction approach?
%As NTPs can learn rules from data, they are related to ILP systems such as FOIL [32], Sherlock [57] and meta-interpretive learning of higher-order dyadic Datalog (Metagol) [58]. While these ILP systems operate on symbols and search over the discrete space of logical rules, NTPs work with subsymbolic representations and induce rules using gradient descent. 
%Recently, [37] introduced a differentiable rule learning system based on TensorLog and a neural network controller similar to LSTMs 

\veronika{neural lp:is `one of the first' true?} 

\veronika{FOIL: I only coarsely read the paper and did not find the point where it describes which head predicates are considered for rule construction. all possible ones? it says it starts with the target predicate}

\subsection{Implementation}
We ran all systems on the same machine, a ...

\veronika{mention specific configuration params we use with below links?} 
FOIL\footnote{\url{http://www.cs.cmu.edu/afs/cs/project/ai-repository-9/ai/util/areas/learning/systems/foil/foil5/0.html}} (version 5)
AMIE+\footnote{\url{https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/amie/}} (version of 2015-08-26)
And code of following (current date)...
Neural LP\footnote{\url{https://github.com/fanyangxyz/Neural-LP}}
NTP\footnote{\url{https://github.com/uclmr/ntp}}
The scripts for running the evaluation are available in our GitHub repository.

\veronika{mention negative examples given to FOIL and how we handle validation facts for ntp... and in general}

Since Neural LP and AMIE+ only support binary atoms, we represent unary atoms of forms $p(X)$ and $p(c)$ by $p(X,X)$ and $p(c,c)$, respectively, as it is usual \cite{EGre-jair18:learning-explanatory-rules}.

NTP only learns rules whose general format corresponds to so-called \emph{templates} it is given as input. They are abstracted rules, such as $\#1(X, Y) \ass \#2(X, Z), \#2(Z, Y) $, which specify the number and variables of atoms and which predicates are shared among them ($\#2$ in the example). In order to not restrict ntp, we provide it with all templates that are necessary to learn the rules in our datasets.

\subsection{Results}



\subsection{Discussion}

\veronika{add general discussion}

Note that FOIL generally learns rules as introduced in Section~\ref{sec:motivation}, but all other systems consider only a restricted subset. 
\veronika{the 2 of us need to discuss if this is true wrt amie and ntp. do they really NOT consider existential quantification in rule heads? I think they do not.}
As a consequence, we can directly determine which of our rules cannot be (fully) detected by specific systems at all. 
Consider, for example, the rules in Figure~\ref{fig:datasets}.
% 
AMIE+ does not consider reflexive rules and 
%not entirely sure if this is true and not relevant for us: considers at most two body atoms per rule, 
%seems that amie only considers binary atoms (only those occur in paper), then connected + closed restriction yield that it does not consider existential variables in head, ie datalog, no?
requires rules to be connected and closed.
The latter condition yields that it can find only one rule in CHAIN-S and only two rules in DRDG-XS; in RDG-S, all rules are connected and closed.
\veronika{do the results confirm this? mention!}
\veronika{mention somewhere else (where it fits) that this nicely shows the variability of our datasets}
% 
Neural LP only supports chain rules (without constants),%
\footnote{The rule specification in \cite{YaYaCo-NIPS17:neurallp} is slightly ambiguous w.r.t.\ the predicate and variable names but the system's output shows that it learns chain rules as they are defined in Section~\ref{sec:motivation}.}
which do not occur in CHAIN-S, only twice in DRDG-XS, but four times in RDG-S.
\veronika{I have trouble with the chain rule example in the neural lp paper:
\\query(Y,X):-Rn(Y,Zn),...,R1(Z1,X)\\
shouldn't it be 
\\query(Y,X):-Rn(Y,Zn-1),...,R1(Z1,X)?\\
so that we really get a chain as  
\\query(Y,X):-Rn(Y,Z3),R3(Z3,Z2),R2(Z2,Z1),R1(Z1,X)?\\
where $n\neq3$!}

Nevertheless, rules that are not fully supported can still be recognized partially.
For instance, Neural LP learns chain rules
$p_{10}(X_0,X_1) \ass p_5(X_0,X_1)$ and
$p_6(X_0,X_1) \ass p_8(X_0,X_1)$ for datasets CHAIN-S and DRDG-XS, respectivly; though, their confidences are not the highest but rather average among the rules learned.

Recall that we provide all necessary templates for NTP, thus it is not restricted in this regard. %not sure but it seems that ntp only considers datalog, ie no existentially quantified varoables in head. do you know this? just out of curiosity...

\veronika{do the result measures confirm this?}

%\veronika{compare results on our datasets to results on existing datasets/evaluations}
%The related work discussed below focuses on learning first-order rules.